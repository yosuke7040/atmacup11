{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "opposed-poverty",
   "metadata": {},
   "source": [
    "## SimSiamの写経  \n",
    "https://www.guruguru.science/competitions/17/discussions/a39d588e-aff2-4728-8323-b07f15563552/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "major-purse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import lightly\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "instructional-yesterday",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 4\n",
    "batch_size = 128\n",
    "seed = 1993\n",
    "epochs = 50\n",
    "input_size = 224\n",
    "\n",
    "# dimension of the embeddings\n",
    "num_ftrs = 512\n",
    "# dimension of the output of the prediction and projection heads\n",
    "out_dim = proj_hidden_dim = 512\n",
    "# the prediction head uses a bottleneck architecture\n",
    "#pred_hidden_dim = 128\n",
    "# use 2 layers in the projection head\n",
    "num_mlp_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "lasting-addiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "input_dir = '/src/atmacup/atmacup11/data/inputs/'\n",
    "path_to_data = os.path.join(input_dir, 'photos')\n",
    "model_dir = '/src/atmacup/atmacup11/data/model/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-taiwan",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "elegant-lesbian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the augmentations for self-supervised learning\n",
    "collate_fn = lightly.data.ImageCollateFunction(\n",
    "    input_size=input_size,\n",
    "    # require invariance to flips and rotations\n",
    "    hf_prob=0.5,\n",
    "    vf_prob=0.5,\n",
    "    rr_prob=0.5,\n",
    "    # satellite images are all taken from the same height\n",
    "    # so we use only slight random cropping\n",
    "    min_scale=0.5,\n",
    "    # use a weak color jitter for invariance w.r.t small color changes\n",
    "    cj_prob=0.2,\n",
    "    cj_bright=0.1,\n",
    "    cj_contrast=0.1,\n",
    "    cj_hue=0.1,\n",
    "    cj_sat=0.1,\n",
    ")\n",
    "\n",
    "# create a lightly dataset for training, since the augmentations are handled\n",
    "# by the collate function, there is no need to apply additional ones here\n",
    "dataset_train_simsiam = lightly.data.LightlyDataset(\n",
    "    input_dir=path_to_data\n",
    ")\n",
    "\n",
    "# create a dataloader for training\n",
    "dataloader_train_simsiam = torch.utils.data.DataLoader(\n",
    "    dataset_train_simsiam,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "# create a torchvision transformation for embedding the dataset after training\n",
    "# here, we resize the images to match the input size during training and apply\n",
    "# a normalization of the color channel based on statistics from imagenet\n",
    "test_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((input_size, input_size)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(\n",
    "        mean=lightly.data.collate.imagenet_normalize['mean'],\n",
    "        std=lightly.data.collate.imagenet_normalize['std'],\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# create a lightly dataset for embedding\n",
    "dataset_test = lightly.data.LightlyDataset(\n",
    "    input_dir=path_to_data,\n",
    "    transform=test_transforms\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# create a dataloader for embedding\n",
    "dataloader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-apollo",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "super-understanding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use a pretrained resnet for this tutorial to speed\n",
    "# up training time but you can also train one from scratch\n",
    "# Do not use pretrained Model\n",
    "resnet = torchvision.models.resnet18(pretrained=False)\n",
    "backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "\n",
    "# create the SimSiam model using the backbone from above\n",
    "model = lightly.models.SimSiam(\n",
    "    backbone,\n",
    "    num_ftrs=num_ftrs,\n",
    "#     proj_hidden_dim=pred_hidden_dim,\n",
    "#     pred_hidden_dim=pred_hidden_dim,\n",
    "#     out_dim=out_dim,\n",
    "    num_mlp_layers=num_mlp_layers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-penguin",
   "metadata": {},
   "source": [
    "## Loss / Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "enormous-calendar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimSiam uses a symmetric negative cosine similarity loss\n",
    "criterion = lightly.loss.SymNegCosineSimilarityLoss()\n",
    "\n",
    "# scale the learning rate\n",
    "lr = 0.05 * batch_size / 256\n",
    "# use SGD with momentum and weight decay\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=lr,\n",
    "    momentum=0.9,\n",
    "    weight_decay=5e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-emission",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch   0] Loss = -0.73 | Collapse Level: 0.55 / 1.00\n",
      "[Epoch   1] Loss = -0.84 | Collapse Level: 0.56 / 1.00\n",
      "[Epoch   2] Loss = -0.85 | Collapse Level: 0.55 / 1.00\n",
      "[Epoch   3] Loss = -0.87 | Collapse Level: 0.55 / 1.00\n",
      "[Epoch   4] Loss = -0.88 | Collapse Level: 0.55 / 1.00\n",
      "[Epoch   5] Loss = -0.90 | Collapse Level: 0.56 / 1.00\n",
      "[Epoch   6] Loss = -0.90 | Collapse Level: 0.56 / 1.00\n",
      "[Epoch   7] Loss = -0.91 | Collapse Level: 0.56 / 1.00\n",
      "[Epoch   8] Loss = -0.90 | Collapse Level: 0.56 / 1.00\n",
      "[Epoch   9] Loss = -0.90 | Collapse Level: 0.56 / 1.00\n",
      "[Epoch  10] Loss = -0.91 | Collapse Level: 0.56 / 1.00\n",
      "[Epoch  11] Loss = -0.90 | Collapse Level: 0.56 / 1.00\n",
      "[Epoch  12] Loss = -0.91 | Collapse Level: 0.56 / 1.00\n",
      "[Epoch  13] Loss = -0.91 | Collapse Level: 0.56 / 1.00\n",
      "[Epoch  14] Loss = -0.91 | Collapse Level: 0.56 / 1.00\n",
      "[Epoch  15] Loss = -0.91 | Collapse Level: 0.56 / 1.00\n",
      "[Epoch  16] Loss = -0.91 | Collapse Level: 0.56 / 1.00\n",
      "[Epoch  17] Loss = -0.91 | Collapse Level: 0.56 / 1.00\n",
      "[Epoch  18] Loss = -0.91 | Collapse Level: 0.56 / 1.00\n",
      "[Epoch  19] Loss = -0.92 | Collapse Level: 0.56 / 1.00\n",
      "[Epoch  20] Loss = -0.92 | Collapse Level: 0.56 / 1.00\n",
      "[Epoch  21] Loss = -0.91 | Collapse Level: 0.56 / 1.00\n",
      "[Epoch  22] Loss = -0.91 | Collapse Level: 0.56 / 1.00\n",
      "[Epoch  23] Loss = -0.91 | Collapse Level: 0.56 / 1.00\n",
      "[Epoch  24] Loss = -0.92 | Collapse Level: 0.56 / 1.00\n",
      "[Epoch  25] Loss = -0.91 | Collapse Level: 0.56 / 1.00\n",
      "[Epoch  26] Loss = -0.91 | Collapse Level: 0.56 / 1.00\n",
      "[Epoch  27] Loss = -0.91 | Collapse Level: 0.56 / 1.00\n",
      "[Epoch  28] Loss = -0.92 | Collapse Level: 0.56 / 1.00\n",
      "[Epoch  29] Loss = -0.91 | Collapse Level: 0.56 / 1.00\n",
      "[Epoch  30] Loss = -0.92 | Collapse Level: 0.56 / 1.00\n",
      "[Epoch  31] Loss = -0.91 | Collapse Level: 0.56 / 1.00\n",
      "[Epoch  32] Loss = -0.92 | Collapse Level: 0.56 / 1.00\n",
      "[Epoch  33] Loss = -0.92 | Collapse Level: 0.56 / 1.00\n",
      "[Epoch  34] Loss = -0.92 | Collapse Level: 0.56 / 1.00\n",
      "[Epoch  35] Loss = -0.91 | Collapse Level: 0.56 / 1.00\n",
      "[Epoch  36] Loss = -0.92 | Collapse Level: 0.56 / 1.00\n",
      "[Epoch  37] Loss = -0.92 | Collapse Level: 0.56 / 1.00\n",
      "[Epoch  38] Loss = -0.91 | Collapse Level: 0.56 / 1.00\n",
      "[Epoch  39] Loss = -0.92 | Collapse Level: 0.56 / 1.00\n",
      "[Epoch  40] Loss = -0.92 | Collapse Level: 0.56 / 1.00\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "avg_loss = 0.\n",
    "avg_output_std = 0.\n",
    "for e in range(epochs):\n",
    "\n",
    "    for (x0, x1), _, _ in dataloader_train_simsiam:\n",
    "\n",
    "        # move images to the gpu\n",
    "        x0 = x0.to(device)\n",
    "        x1 = x1.to(device)\n",
    "\n",
    "        # run the model on both transforms of the images\n",
    "        # the output of the simsiam model is a y containing the predictions\n",
    "        # and projections for each input x\n",
    "        y0, y1 = model(x0, x1)\n",
    "\n",
    "        # backpropagation\n",
    "        loss = criterion(y0, y1)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # calculate the per-dimension standard deviation of the outputs\n",
    "        # we can use this later to check whether the embeddings are collapsing\n",
    "        output, _ = y0\n",
    "        output = output.detach()\n",
    "        output = torch.nn.functional.normalize(output, dim=1)\n",
    "\n",
    "        output_std = torch.std(output, 0)\n",
    "        output_std = output_std.mean()\n",
    "\n",
    "        # use moving averages to track the loss and standard deviation\n",
    "        w = 0.9\n",
    "        avg_loss = w * avg_loss + (1 - w) * loss.item()\n",
    "        avg_output_std = w * avg_output_std + (1 - w) * output_std.item()\n",
    "\n",
    "    # the level of collapse is large if the standard deviation of the l2\n",
    "    # normalized output is much smaller than 1 / sqrt(dim)\n",
    "    collapse_level = max(0., 1 - math.sqrt(out_dim) * avg_output_std)\n",
    "    # print intermediate results\n",
    "    print(f'[Epoch {e:3d}] '\n",
    "        f'Loss = {avg_loss:.2f} | '\n",
    "        f'Collapse Level: {collapse_level:.2f} / 1.00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-situation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-magic",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(model_dir,'simsiam_res18.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-discussion",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(model_dir,'simsiam_res18.pth')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-niger",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
